{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Classification with Naive Bayes\n",
    "***\n",
    "In the mini-project, you'll learn the basics of text analysis using a subset of movie reviews from the rotten tomatoes database. You'll also use a fundamental technique in Bayesian inference, called Naive Bayes. This mini-project is based on [Lab 10 of Harvard's CS109](https://github.com/cs109/2015lab10) class.  Please free to go to the original lab for additional exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from six.moves import range\n",
    "\n",
    "# Setup Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Rotten Tomatoes Dataset](#Rotten-Tomatoes-Dataset)\n",
    "    * [Explore](#Explore)\n",
    "* [The Vector Space Model and a Search Engine](#The-Vector-Space-Model-and-a-Search-Engine)\n",
    "    * [In Code](#In-Code)\n",
    "* [Naive Bayes](#Naive-Bayes)\n",
    "    * [Multinomial Naive Bayes and Other Likelihood Functions](#Multinomial-Naive-Bayes-and-Other-Likelihood-Functions)\n",
    "    * [Picking Hyperparameters for Naive Bayes and Text Maintenance](#Picking-Hyperparameters-for-Naive-Bayes-and-Text-Maintenance)\n",
    "* [Interpretation](#Interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb     publication                                              quote review_date  rtid      title\n",
       "1         Derek Adams  fresh  114709        Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story\n",
       "2     Richard Corliss  fresh  114709   TIME Magazine                  The year's most inventive comedy.  2008-08-31  9559  Toy story\n",
       "3         David Ansen  fresh  114709        Newsweek  A winning animated feature that has something ...  2008-08-18  9559  Toy story\n",
       "4       Leonard Klady  fresh  114709         Variety  The film sports a provocative and appealing st...  2008-06-09  9559  Toy story\n",
       "5  Jonathan Rosenbaum  fresh  114709  Chicago Reader  An entertaining computer-generated, hyperreali...  2008-03-10  9559  Toy story"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics = pd.read_csv('./critics.csv')\n",
    "#let's drop rows with missing quotes\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 15561\n",
      "Number of critics: 623\n",
      "Number of movies:  1921\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len(critics)\n",
    "n_movies = critics.rtid.unique().size\n",
    "n_critics = critics.critic.unique().size\n",
    "\n",
    "\n",
    "print(\"Number of reviews: {:d}\".format(n_reviews))\n",
    "print(\"Number of critics: {:d}\".format(n_critics))\n",
    "print(\"Number of movies:  {:d}\".format(n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGLCAYAAADONjiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMFv/Azcz6rsD9rcrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d71fff8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = critics.copy()\n",
    "df['fresh'] = df.fresh == 'fresh'\n",
    "grp = df.groupby('critic')\n",
    "counts = grp.critic.count()  # number of reviews by each critic\n",
    "means = grp.fresh.mean()     # average freshness for each critic\n",
    "\n",
    "means[counts > 100].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average Rating per critic\")\n",
    "plt.ylabel(\"Number of Critics\")\n",
    "plt.yticks([0, 2, 4, 6, 8, 10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set I</h3>\n",
    "<br/>\n",
    "<b>Exercise:</b> Look at the histogram above. Tell a story about the average ratings per critic. What shape does the distribution look like? What is interesting about the distribution? What might explain these interesting things?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The histogram above tells us that the data although having a normal distribution is slightly left-tailed (left-skewed) i.e. the avg. lower ratings per critic are critiqued by fewer critics.  In other words fewer critics provide lower ratings and more have a positive outlook and provide higher ratings so this causes a skew or bias in the dataset.  The dip at the 0.55 rating shows few critics want to give the 0.5 rating and are either harsher or lenient in their ratings.\n",
    "###### The center/median is 0.6 rating, the spread is 0.35-0.81.\n",
    "###### There are no true outliers but the data below 0.45 avg. rating could be considered as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space Model and a Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the diagrams here are snipped from [*Introduction to Information Retrieval* by Manning et. al.]( http://nlp.stanford.edu/IR-book/) which is a great resource on text processing. For additional information on text mining and natural language processing, see [*Foundations of Statistical Natural Language Processing* by Manning and Schutze](http://nlp.stanford.edu/fsnlp/).\n",
    "\n",
    "Also check out Python packages [`nltk`](http://www.nltk.org/), [`spaCy`](https://spacy.io/), [`pattern`](http://www.clips.ua.ac.be/pattern), and their associated resources. Also see [`word2vec`](https://en.wikipedia.org/wiki/Word2vec).\n",
    "\n",
    "Let us define the vector derived from document $d$ by $\\bar V(d)$. What does this mean? Each document is treated as a vector containing information about the words contained in it. Each vector has the same length and each entry \"slot\" in the vector contains some kind of data about the words that appear in the document such as presence/absence (1/0), count (an integer) or some other statistic. Each vector has the same length because each document shared the same vocabulary across the full collection of documents -- this collection is called a *corpus*.\n",
    "\n",
    "To define the vocabulary, we take a union of all words we have seen in all documents. We then just associate an array index with them. So \"hello\" may be at index 5 and \"world\" at index 99.\n",
    "\n",
    "Suppose we have the following corpus:\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree. The grapes seemed ready to burst with juice, and the Fox's mouth watered as he gazed longingly at them.`\n",
    "\n",
    "Suppose we treat each sentence as a document $d$. The vocabulary (often called the *lexicon*) is the following:\n",
    "\n",
    "$V = \\left\\{\\right.$ `a, along, and, as, at, beautiful, branches, bunch, burst, day, fox, fox's, from, gazed, grapes, hanging, he, juice, longingly, mouth, of, one, ready, ripe, seemed, spied, the, them, to, trained, tree, vine, watered, with`$\\left.\\right\\}$\n",
    "\n",
    "Then the document\n",
    "\n",
    "`A Fox one day spied a beautiful bunch of ripe grapes hanging from a vine trained along the branches of a tree`\n",
    "\n",
    "may be represented as the following sparse vector of word counts:\n",
    "\n",
    "$$\\bar V(d) = \\left( 4,1,0,0,0,1,1,1,0,1,1,0,1,0,1,1,0,0,0,0,2,1,0,1,0,0,1,0,0,0,1,1,0,0 \\right)$$\n",
    "\n",
    "or more succinctly as\n",
    "\n",
    "`[(0, 4), (1, 1), (5, 1), (6, 1), (7, 1), (9, 1), (10, 1), (12, 1), (14, 1), (15, 1), (20, 2), (21, 1), (23, 1),`\n",
    "`(26, 1), (30, 1), (31, 1)]`\n",
    "\n",
    "along with a dictionary\n",
    "\n",
    "``\n",
    "{\n",
    "    0: a, 1: along, 5: beautiful, 6: branches, 7: bunch, 9: day, 10: fox, 12: from, 14: grapes, \n",
    "    15: hanging, 19: mouth, 20: of, 21: one, 23: ripe, 24: seemed, 25: spied, 26: the, \n",
    "    30: tree, 31: vine, \n",
    "}\n",
    "``\n",
    "\n",
    "Then, a set of documents becomes, in the usual `sklearn` style, a sparse matrix with rows being sparse arrays representing documents and columns representing the features/words in the vocabulary.\n",
    "\n",
    "Notice that this representation loses the relative ordering of the terms in the document. That is \"cat ate rat\" and \"rat ate cat\" are the same. Thus, this representation is also known as the Bag-Of-Words representation.\n",
    "\n",
    "Here is another example, from the book quoted above, although the matrix is transposed here so that documents are columns:\n",
    "\n",
    "![novel terms](terms.png)\n",
    "\n",
    "Such a matrix is also catted a Term-Document Matrix. Here, the terms being indexed could be stemmed before indexing; for instance, `jealous` and `jealousy` after stemming are the same feature. One could also make use of other \"Natural Language Processing\" transformations in constructing the vocabulary. We could use Lemmatization, which reduces words to lemmas: work, working, worked would all reduce to work. We could remove \"stopwords\" from our vocabulary, such as common words like \"the\". We could look for particular parts of speech, such as adjectives. This is often done in Sentiment Analysis. And so on. It all depends on our application.\n",
    "\n",
    "From the book:\n",
    ">The standard way of quantifying the similarity between two documents $d_1$ and $d_2$  is to compute the cosine similarity of their vector representations $\\bar V(d_1)$ and $\\bar V(d_2)$:\n",
    "\n",
    "$$S_{12} = \\frac{\\bar V(d_1) \\cdot \\bar V(d_2)}{|\\bar V(d_1)| \\times |\\bar V(d_2)|}$$\n",
    "\n",
    "![Vector Space Model](vsm.png)\n",
    "\n",
    "\n",
    ">There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q = jealous gossip. This query turns into the unit vector $\\bar V(q)$ = (0, 0.707, 0.707) on the three coordinates below. \n",
    "\n",
    "![novel terms](terms2.png)\n",
    "\n",
    ">The key idea now: to assign to each document d a score equal to the dot product:\n",
    "\n",
    "$$\\bar V(q) \\cdot \\bar V(d)$$\n",
    "\n",
    "Then we can use this simple Vector Model as a Search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is\n",
      "Hop on pop\n",
      "Hop off pop\n",
      "Hop Hop hop\n",
      "\n",
      "Transformed text vector is \n",
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [3 0 0 0]]\n",
      "\n",
      "Words for each feature:\n",
      "['hop', 'off', 'on', 'pop']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['Hop on pop', 'Hop off pop', 'Hop Hop hop']\n",
    "print(\"Original text is\\n{}\".format('\\n'.join(text)))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Transformed text vector is \\n{}\".format(x))\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print(\"\")\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
    "# just their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1],\n",
       "       [1, 1, 0, 1],\n",
       "       [3, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hop</th>\n",
       "      <th>off</th>\n",
       "      <th>on</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hop  off  on  pop\n",
       "0    1    0   1    1\n",
       "1    1    1   0    1\n",
       "2    3    0   0    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        So ingenious in concept, design and execution ...\n",
      "2                        The year's most inventive comedy.\n",
      "3        A winning animated feature that has something ...\n",
      "4        The film sports a provocative and appealing st...\n",
      "5        An entertaining computer-generated, hyperreali...\n",
      "6        As Lion King did before it, Toy Story revived ...\n",
      "7        The film will probably be more fully appreciat...\n",
      "8        Children will enjoy a new take on the irresist...\n",
      "9        Although its computer-generated imagery is imp...\n",
      "10       How perfect that two of the most popular funny...\n",
      "11       The result is a visionary roller-coaster ride ...\n",
      "12       Disney's witty, wondrously imaginative, all-co...\n",
      "13       Disney's first computer-made animated feature ...\n",
      "14       The script, by Lasseter, Pete Docter, Andrew S...\n",
      "15       The one big negative about Toy Story involves ...\n",
      "16              Technically, Toy Story is nearly flawless.\n",
      "17       It's a nice change of pace to see the studio d...\n",
      "18       I can hardly imagine having more fun at the mo...\n",
      "19       The great voice acting, the visual puns, all a...\n",
      "30       A gloomy special-effects extravaganza filled w...\n",
      "32       A calculated but very entertaining special eff...\n",
      "36       Walter Matthau and Jack Lemmon are awfully goo...\n",
      "37                                  Mediocre, regrettably.\n",
      "38       Just don't expect their bickering to be on the...\n",
      "39       While it won't come close to my top 10 best li...\n",
      "40       The movie is too pat and practiced to really b...\n",
      "41       If you poke through Grumpy's cheap sentimental...\n",
      "47       Never escapes the queasy aura of Melrose Place...\n",
      "48       A pleasant if undemanding piece of work that i...\n",
      "49       You want the movie to stomp and rejoice and cr...\n",
      "                               ...                        \n",
      "27560    Although most of this is rather familiar stuff...\n",
      "27561    This isn't an adolescent wish-fulfillment fant...\n",
      "27563    A complicated film that never really successfu...\n",
      "27566    Robert Aldrich's \"daring\" 1968 mating of lesbi...\n",
      "27570    A little windy and rhetorical for my taste, bu...\n",
      "27571    Emphasis is on tension in the telling, and eff...\n",
      "27572    Although the characters are basically stereoty...\n",
      "27573    The script prepared by Mr. Huston and Richard ...\n",
      "27574                            Disturbing and affecting.\n",
      "27575    Claustrophobic and overwrought, Jailbait is an...\n",
      "27576    While the stars deliver highly committed perfo...\n",
      "27577    A stagy, only mildly compelling prison drama t...\n",
      "27578    I wouldn't have thought it was possible to mak...\n",
      "27579    The cruelty of the law has been better demonst...\n",
      "27580    ... the umpteenth prison drama to focus on the...\n",
      "27593    Mr. Peckinpah's least interesting, least perso...\n",
      "27595    George Cukor directed, a little impersonally f...\n",
      "27596                                               Magic.\n",
      "27597    More firm in its social implications than ever...\n",
      "27598    Belying the lightheartedness of its title, Bir...\n",
      "27599    A trifle self-indulgent -- well, it is directe...\n",
      "27600                  A very strange and beautiful movie.\n",
      "27601                         Most of Birdy is enchanting.\n",
      "27603    We can't recommend this little item as a sampl...\n",
      "27605    Ballard and his masterly crew of film makers h...\n",
      "27606    Measures up to the promise Ballard amply provi...\n",
      "27607    For the most part very absorbing, the film suf...\n",
      "27608    Perhaps the best thing about the film is that ...\n",
      "27609    The film is still memorable for its compassion...\n",
      "27616    It does have enough gritty insights and (for t...\n",
      "Name: quote, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(critics.quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n"
     ]
    }
   ],
   "source": [
    "def make_xy(critics, vectorizer=None):\n",
    "    #Your code here    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(critics.quote)\n",
    "    print(X)\n",
    "\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    print(X)\n",
    "    y = (critics.fresh == 'fresh').values.astype(np.int)\n",
    "    return X, y\n",
    "X, y = make_xy(critics)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes' Theorem, we have that\n",
    "\n",
    "$$P(c \\vert f) = \\frac{P(c \\cap f)}{P(f)}$$\n",
    "\n",
    "where $c$ represents a *class* or category, and $f$ represents a feature vector, such as $\\bar V(d)$ as above. **We are computing the probability that a document (or whatever we are classifying) belongs to category *c* given the features in the document.** $P(f)$ is really just a normalization constant, so the literature usually writes Bayes' Theorem in context of Naive Bayes as\n",
    "\n",
    "$$P(c \\vert f) \\propto P(f \\vert c) P(c) $$\n",
    "\n",
    "$P(c)$ is called the *prior* and is simply the probability of seeing class $c$. But what is $P(f \\vert c)$? This is the probability that we see feature set $f$ given that this document is actually in class $c$. This is called the *likelihood* and comes from the data. One of the major assumptions of the Naive Bayes model is that the features are *conditionally independent* given the class. While the presence of a particular discriminative word may uniquely identify the document as being part of class $c$ and thus violate general feature independence, conditional independence means that the presence of that term is independent of all the other words that appear *within that class*. This is a very important distinction. Recall that if two events are independent, then:\n",
    "\n",
    "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
    "\n",
    "Thus, conditional independence implies\n",
    "\n",
    "$$P(f \\vert c)  = \\prod_i P(f_i | c) $$\n",
    "\n",
    "where $f_i$ is an individual feature (a word in this example).\n",
    "\n",
    "To make a classification, we then choose the class $c$ such that $P(c \\vert f)$ is maximal.\n",
    "\n",
    "There is a small caveat when computing these probabilities. For [floating point underflow](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html) we change the product into a sum by going into log space. This is called the LogSumExp trick. So:\n",
    "\n",
    "$$\\log P(f \\vert c)  = \\sum_i \\log P(f_i \\vert c) $$\n",
    "\n",
    "There is another caveat. What if we see a term that didn't exist in the training data? This means that $P(f_i \\vert c) = 0$ for that term, and thus $P(f \\vert c)  = \\prod_i P(f_i | c) = 0$, which doesn't help us at all. Instead of using zeros, we add a small negligible value called $\\alpha$ to each count. This is called Laplace Smoothing.\n",
    "\n",
    "$$P(f_i \\vert c) = \\frac{N_{ic}+\\alpha}{N_c + \\alpha N_i}$$\n",
    "\n",
    "where $N_{ic}$ is the number of times feature $i$ was seen in class $c$, $N_c$ is the number of times class $c$ was seen and $N_i$ is the number of times feature $i$ was seen globally. $\\alpha$ is sometimes called a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes and Other Likelihood Functions\n",
    "\n",
    "Since we are modeling word counts, we are using variation of Naive Bayes called Multinomial Naive Bayes. This is because the likelihood function actually takes the form of the multinomial distribution.\n",
    "\n",
    "$$P(f \\vert c) = \\frac{\\left( \\sum_i f_i \\right)!}{\\prod_i f_i!} \\prod_{f_i} P(f_i \\vert c)^{f_i} \\propto \\prod_{i} P(f_i \\vert c)$$\n",
    "\n",
    "where the nasty term out front is absorbed as a normalization constant such that probabilities sum to 1.\n",
    "\n",
    "There are many other variations of Naive Bayes, all which depend on what type of value $f_i$ takes. If $f_i$ is continuous, we may be able to use *Gaussian Naive Bayes*. First compute the mean and variance for each class $c$. Then the likelihood, $P(f \\vert c)$ is given as follows\n",
    "\n",
    "$$P(f_i = v \\vert c) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_c}} e^{- \\frac{\\left( v - \\mu_c \\right)^2}{2 \\sigma^2_c}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set II</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Implement a simple Naive Bayes classifier:</p>\n",
    "\n",
    "<ol>\n",
    "<li> split the data set into a training and test set\n",
    "<li> Use `scikit-learn`'s `MultinomialNB()` classifier with default parameters.\n",
    "<li> train the classifier over the training set and test on the test set\n",
    "<li> print the accuracy scores for both the training and the test sets\n",
    "</ol>\n",
    "\n",
    "What do you notice? Is this a good classifier? If not, why not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from the training data: 0.919\n",
      "Accuracy from the test data: 0.779\n"
     ]
    }
   ],
   "source": [
    "#your turn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xnb, Xtestnb, ynb, ytestnb = train_test_split(X,y, test_size=0.2,random_state=5)\n",
    "\n",
    "# print(Xnb)\n",
    "# print(ynb)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(Xnb,ynb)\n",
    "# Print the accuracy from the training data.\n",
    "print(\"Accuracy from the training data: {:0.3f}\".format(clf.score(Xnb, ynb))) \n",
    "# Print the accuracy from the testing data.\n",
    "# print(accuracy_score(clf.predict(Xtestnb), ytestnb))   \n",
    "print(\"Accuracy from the test data: {:0.3f}\".format(clf.score(Xtestnb, ytestnb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is a moderately good classifier.  There maybe a concern of overfitting the data. The accuracy score has deteriorated by 14% from the training data to the testing data. showing that there maybe a way to improve this model to perform better on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Hyperparameters for Naive Bayes and Text Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what value to use for $\\alpha$, and we also need to know which words to include in the vocabulary. As mentioned earlier, some words are obvious stopwords. Other words appear so infrequently that they serve as noise, and other words in addition to stopwords appear so frequently that they may also serve as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find an appropriate value for `min_df` for the `CountVectorizer`. `min_df` can be either an integer or a float/decimal. If it is an integer, `min_df` represents the minimum number of documents a word must appear in for it to be included in the vocabulary. If it is a float, it represents the minimum *percentage* of documents a word must appear in to be included in the vocabulary. From the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set III</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Construct the cumulative distribution of document frequencies (df). The $x$-axis is a document count $x_i$ and the $y$-axis is the percentage of words that appear less than $x_i$ times. For example, at $x=5$, plot a point representing the percentage or number of words that appear in 5 or fewer documents.</p>\n",
    "\n",
    "<p><b>Exercise:</b> Look for the point at which the curve begins climbing steeply. This may be a good value for `min_df`. If we were interested in also picking `max_df`, we would likely pick the value where the curve starts to plateau. What value did you choose?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<12448x22417 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 217842 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Xnb.toarray())\n",
    "Xnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hop', 'off', 'on', 'pop']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = list(sorted((X > 0).sum(axis=0).reshape(-1).tolist()[0]))\n",
    "rows, features = X.shape\n",
    "height, axis = np.histogram(df, bins=len(np.unique(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 78, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 84, 84, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 87, 87, 88, 88, 88, 88, 89, 89, 89, 90, 90, 90, 91, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 95, 95, 96, 96, 96, 97, 97, 97, 97, 98, 98, 99, 99, 99, 99, 100, 100, 100, 100, 101, 101, 102, 102, 102, 102, 103, 103, 104, 104, 104, 105, 105, 105, 106, 106, 106, 106, 106, 107, 107, 107, 107, 108, 108, 109, 110, 110, 110, 110, 111, 111, 111, 111, 112, 113, 113, 113, 113, 113, 115, 115, 115, 115, 115, 115, 115, 116, 116, 116, 116, 116, 119, 120, 120, 121, 121, 123, 123, 126, 126, 126, 127, 128, 128, 129, 129, 131, 131, 132, 132, 133, 133, 134, 137, 137, 137, 138, 138, 138, 139, 139, 140, 140, 141, 141, 141, 141, 141, 143, 144, 144, 144, 144, 145, 146, 146, 146, 146, 149, 149, 153, 154, 154, 154, 154, 154, 154, 154, 155, 155, 155, 156, 156, 158, 160, 160, 162, 165, 165, 166, 166, 169, 169, 170, 171, 172, 172, 172, 174, 174, 175, 175, 176, 177, 177, 182, 183, 186, 187, 187, 191, 193, 194, 194, 195, 196, 197, 198, 198, 198, 199, 199, 199, 201, 202, 202, 207, 208, 210, 210, 211, 211, 212, 214, 215, 216, 218, 218, 220, 220, 221, 223, 224, 227, 228, 228, 228, 230, 231, 232, 234, 235, 237, 238, 241, 242, 243, 245, 246, 248, 249, 249, 249, 250, 250, 251, 252, 253, 255, 256, 257, 258, 259, 263, 265, 269, 269, 271, 272, 272, 274, 274, 275, 277, 278, 280, 283, 286, 288, 295, 299, 301, 310, 312, 315, 316, 320, 327, 329, 330, 335, 342, 349, 353, 360, 361, 362, 373, 374, 384, 385, 386, 388, 398, 400, 401, 408, 412, 414, 423, 425, 427, 431, 445, 460, 461, 469, 484, 490, 501, 517, 525, 529, 563, 579, 582, 594, 601, 628, 645, 646, 647, 654, 655, 674, 678, 694, 752, 774, 785, 824, 839, 945, 948, 951, 973, 1008, 1011, 1041, 1056, 1064, 1173, 1217, 1234, 1254, 1307, 1319, 1661, 1739, 1966, 1987, 2128, 2205, 2260, 2359, 2360, 3290, 3672, 4312, 4957, 5120, 7133, 7552, 9502]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15561, 22417)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 3.12579618e+01, 6.15159236e+01, 9.17738854e+01,\n",
       "       1.22031847e+02, 1.52289809e+02, 1.82547771e+02, 2.12805732e+02,\n",
       "       2.43063694e+02, 2.73321656e+02, 3.03579618e+02, 3.33837580e+02,\n",
       "       3.64095541e+02, 3.94353503e+02, 4.24611465e+02, 4.54869427e+02,\n",
       "       4.85127389e+02, 5.15385350e+02, 5.45643312e+02, 5.75901274e+02,\n",
       "       6.06159236e+02, 6.36417197e+02, 6.66675159e+02, 6.96933121e+02,\n",
       "       7.27191083e+02, 7.57449045e+02, 7.87707006e+02, 8.17964968e+02,\n",
       "       8.48222930e+02, 8.78480892e+02, 9.08738854e+02, 9.38996815e+02,\n",
       "       9.69254777e+02, 9.99512739e+02, 1.02977070e+03, 1.06002866e+03,\n",
       "       1.09028662e+03, 1.12054459e+03, 1.15080255e+03, 1.18106051e+03,\n",
       "       1.21131847e+03, 1.24157643e+03, 1.27183439e+03, 1.30209236e+03,\n",
       "       1.33235032e+03, 1.36260828e+03, 1.39286624e+03, 1.42312420e+03,\n",
       "       1.45338217e+03, 1.48364013e+03, 1.51389809e+03, 1.54415605e+03,\n",
       "       1.57441401e+03, 1.60467197e+03, 1.63492994e+03, 1.66518790e+03,\n",
       "       1.69544586e+03, 1.72570382e+03, 1.75596178e+03, 1.78621975e+03,\n",
       "       1.81647771e+03, 1.84673567e+03, 1.87699363e+03, 1.90725159e+03,\n",
       "       1.93750955e+03, 1.96776752e+03, 1.99802548e+03, 2.02828344e+03,\n",
       "       2.05854140e+03, 2.08879936e+03, 2.11905732e+03, 2.14931529e+03,\n",
       "       2.17957325e+03, 2.20983121e+03, 2.24008917e+03, 2.27034713e+03,\n",
       "       2.30060510e+03, 2.33086306e+03, 2.36112102e+03, 2.39137898e+03,\n",
       "       2.42163694e+03, 2.45189490e+03, 2.48215287e+03, 2.51241083e+03,\n",
       "       2.54266879e+03, 2.57292675e+03, 2.60318471e+03, 2.63344268e+03,\n",
       "       2.66370064e+03, 2.69395860e+03, 2.72421656e+03, 2.75447452e+03,\n",
       "       2.78473248e+03, 2.81499045e+03, 2.84524841e+03, 2.87550637e+03,\n",
       "       2.90576433e+03, 2.93602229e+03, 2.96628025e+03, 2.99653822e+03,\n",
       "       3.02679618e+03, 3.05705414e+03, 3.08731210e+03, 3.11757006e+03,\n",
       "       3.14782803e+03, 3.17808599e+03, 3.20834395e+03, 3.23860191e+03,\n",
       "       3.26885987e+03, 3.29911783e+03, 3.32937580e+03, 3.35963376e+03,\n",
       "       3.38989172e+03, 3.42014968e+03, 3.45040764e+03, 3.48066561e+03,\n",
       "       3.51092357e+03, 3.54118153e+03, 3.57143949e+03, 3.60169745e+03,\n",
       "       3.63195541e+03, 3.66221338e+03, 3.69247134e+03, 3.72272930e+03,\n",
       "       3.75298726e+03, 3.78324522e+03, 3.81350318e+03, 3.84376115e+03,\n",
       "       3.87401911e+03, 3.90427707e+03, 3.93453503e+03, 3.96479299e+03,\n",
       "       3.99505096e+03, 4.02530892e+03, 4.05556688e+03, 4.08582484e+03,\n",
       "       4.11608280e+03, 4.14634076e+03, 4.17659873e+03, 4.20685669e+03,\n",
       "       4.23711465e+03, 4.26737261e+03, 4.29763057e+03, 4.32788854e+03,\n",
       "       4.35814650e+03, 4.38840446e+03, 4.41866242e+03, 4.44892038e+03,\n",
       "       4.47917834e+03, 4.50943631e+03, 4.53969427e+03, 4.56995223e+03,\n",
       "       4.60021019e+03, 4.63046815e+03, 4.66072611e+03, 4.69098408e+03,\n",
       "       4.72124204e+03, 4.75150000e+03, 4.78175796e+03, 4.81201592e+03,\n",
       "       4.84227389e+03, 4.87253185e+03, 4.90278981e+03, 4.93304777e+03,\n",
       "       4.96330573e+03, 4.99356369e+03, 5.02382166e+03, 5.05407962e+03,\n",
       "       5.08433758e+03, 5.11459554e+03, 5.14485350e+03, 5.17511146e+03,\n",
       "       5.20536943e+03, 5.23562739e+03, 5.26588535e+03, 5.29614331e+03,\n",
       "       5.32640127e+03, 5.35665924e+03, 5.38691720e+03, 5.41717516e+03,\n",
       "       5.44743312e+03, 5.47769108e+03, 5.50794904e+03, 5.53820701e+03,\n",
       "       5.56846497e+03, 5.59872293e+03, 5.62898089e+03, 5.65923885e+03,\n",
       "       5.68949682e+03, 5.71975478e+03, 5.75001274e+03, 5.78027070e+03,\n",
       "       5.81052866e+03, 5.84078662e+03, 5.87104459e+03, 5.90130255e+03,\n",
       "       5.93156051e+03, 5.96181847e+03, 5.99207643e+03, 6.02233439e+03,\n",
       "       6.05259236e+03, 6.08285032e+03, 6.11310828e+03, 6.14336624e+03,\n",
       "       6.17362420e+03, 6.20388217e+03, 6.23414013e+03, 6.26439809e+03,\n",
       "       6.29465605e+03, 6.32491401e+03, 6.35517197e+03, 6.38542994e+03,\n",
       "       6.41568790e+03, 6.44594586e+03, 6.47620382e+03, 6.50646178e+03,\n",
       "       6.53671975e+03, 6.56697771e+03, 6.59723567e+03, 6.62749363e+03,\n",
       "       6.65775159e+03, 6.68800955e+03, 6.71826752e+03, 6.74852548e+03,\n",
       "       6.77878344e+03, 6.80904140e+03, 6.83929936e+03, 6.86955732e+03,\n",
       "       6.89981529e+03, 6.93007325e+03, 6.96033121e+03, 6.99058917e+03,\n",
       "       7.02084713e+03, 7.05110510e+03, 7.08136306e+03, 7.11162102e+03,\n",
       "       7.14187898e+03, 7.17213694e+03, 7.20239490e+03, 7.23265287e+03,\n",
       "       7.26291083e+03, 7.29316879e+03, 7.32342675e+03, 7.35368471e+03,\n",
       "       7.38394268e+03, 7.41420064e+03, 7.44445860e+03, 7.47471656e+03,\n",
       "       7.50497452e+03, 7.53523248e+03, 7.56549045e+03, 7.59574841e+03,\n",
       "       7.62600637e+03, 7.65626433e+03, 7.68652229e+03, 7.71678025e+03,\n",
       "       7.74703822e+03, 7.77729618e+03, 7.80755414e+03, 7.83781210e+03,\n",
       "       7.86807006e+03, 7.89832803e+03, 7.92858599e+03, 7.95884395e+03,\n",
       "       7.98910191e+03, 8.01935987e+03, 8.04961783e+03, 8.07987580e+03,\n",
       "       8.11013376e+03, 8.14039172e+03, 8.17064968e+03, 8.20090764e+03,\n",
       "       8.23116561e+03, 8.26142357e+03, 8.29168153e+03, 8.32193949e+03,\n",
       "       8.35219745e+03, 8.38245541e+03, 8.41271338e+03, 8.44297134e+03,\n",
       "       8.47322930e+03, 8.50348726e+03, 8.53374522e+03, 8.56400318e+03,\n",
       "       8.59426115e+03, 8.62451911e+03, 8.65477707e+03, 8.68503503e+03,\n",
       "       8.71529299e+03, 8.74555096e+03, 8.77580892e+03, 8.80606688e+03,\n",
       "       8.83632484e+03, 8.86658280e+03, 8.89684076e+03, 8.92709873e+03,\n",
       "       8.95735669e+03, 8.98761465e+03, 9.01787261e+03, 9.04813057e+03,\n",
       "       9.07838854e+03, 9.10864650e+03, 9.13890446e+03, 9.16916242e+03,\n",
       "       9.19942038e+03, 9.22967834e+03, 9.25993631e+03, 9.29019427e+03,\n",
       "       9.32045223e+03, 9.35071019e+03, 9.38096815e+03, 9.41122611e+03,\n",
       "       9.44148408e+03, 9.47174204e+03, 9.50200000e+03])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.81\n"
     ]
    }
   ],
   "source": [
    "print(\"{:0.2f}\".format(axis[7,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21347,   537,   179,    96,    44,    36,    27,    24,    23,\n",
       "          12,     8,     7,     6,     7,     4,     4,     2,     3,\n",
       "           1,     4,     1,     5,     3,     0,     1,     2,     0,\n",
       "           2,     0,     0,     0,     3,     1,     2,     2,     1,\n",
       "           0,     0,     1,     0,     2,     1,     0,     2,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1,     0,     0,     1,     0,     0,     0,     0,     0,\n",
       "           0,     1,     1,     0,     0,     0,     0,     1,     0,\n",
       "           1,     0,     1,     0,     0,     2,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     1,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     1,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,     0,     0,     0,     0,     0,     1,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     1,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22417"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21347, 21884, 22063, 22159, 22203, 22239, 22266, 22290, 22313,\n",
       "       22325, 22333, 22340, 22346, 22353, 22357, 22361, 22363, 22366,\n",
       "       22367, 22371, 22372, 22377, 22380, 22380, 22381, 22383, 22383,\n",
       "       22385, 22385, 22385, 22385, 22388, 22389, 22391, 22393, 22394,\n",
       "       22394, 22394, 22395, 22395, 22397, 22398, 22398, 22400, 22400,\n",
       "       22400, 22400, 22400, 22400, 22400, 22400, 22400, 22400, 22400,\n",
       "       22401, 22401, 22401, 22402, 22402, 22402, 22402, 22402, 22402,\n",
       "       22402, 22403, 22404, 22404, 22404, 22404, 22404, 22405, 22405,\n",
       "       22406, 22406, 22407, 22407, 22407, 22409, 22409, 22409, 22409,\n",
       "       22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409,\n",
       "       22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409,\n",
       "       22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409, 22409,\n",
       "       22410, 22410, 22410, 22410, 22410, 22410, 22410, 22410, 22410,\n",
       "       22410, 22410, 22410, 22410, 22411, 22411, 22411, 22411, 22411,\n",
       "       22411, 22411, 22411, 22411, 22411, 22411, 22411, 22411, 22411,\n",
       "       22411, 22411, 22411, 22411, 22411, 22411, 22411, 22412, 22412,\n",
       "       22412, 22412, 22412, 22412, 22412, 22412, 22412, 22412, 22412,\n",
       "       22412, 22412, 22412, 22412, 22412, 22412, 22412, 22412, 22412,\n",
       "       22412, 22413, 22413, 22413, 22413, 22413, 22413, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414, 22414,\n",
       "       22414, 22415, 22415, 22415, 22415, 22415, 22415, 22415, 22415,\n",
       "       22415, 22415, 22415, 22415, 22415, 22415, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416, 22416,\n",
       "       22416, 22416, 22416, 22416, 22416, 22416, 22416, 22417],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(height * 1, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cumhist= np.cumsum(height * 1, axis=0) / features\n",
    "# cumhist= np.cumsum(height * 1, axis=0)\n",
    "# ??cumhist.astype(float,\"0.2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumhist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "axis=np.insert(axis, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumhist = np.insert(cumhist, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1d722849470>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGVCAYAAAAVNkAfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7wkHqZuzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d7203f32e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(axis[:-1], cumhist)\n",
    "# height = np.insert(height, 0, 0)\n",
    "# plt.plot(axis[:-1], height)\n",
    "plt.xlim(-.1,20)\n",
    "plt.xlabel(\"DF\")\n",
    "plt.ylabel(\"CDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### mindf = maxdf =  1 as per the above plot i.e. the elbow in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumhist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\alpha$ is chosen to be a small value that simply avoids having zeros in the probability computations. This value can sometimes be chosen arbitrarily with domain expertise, but we will use K-fold cross validation. In K-fold cross-validation, we divide the data into $K$ non-overlapping parts. We train on $K-1$ of the folds and test on the remaining fold. We then iterate, so that each fold serves as the test fold exactly once. The function `cv_score` performs the K-fold cross-validation algorithm for us, but we need to pass a function that measures the performance of the algorithm on each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-likelihood as the score here in `scorefunc`. The higher the log-likelihood, the better. Indeed, what we do in `cv_score` above is to implement the cross-validation part of `GridSearchCV`.\n",
    "\n",
    "The custom scoring function `scorefunc` allows us to use different metrics depending on the decision risk we care about (precision, accuracy, profit etc.) directly on the validation set. You will often find people using `roc_auc`, precision, recall, or `F1-score` as the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross-validate over the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the train and test masks first, and then we can run the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15561"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda33\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\n",
    "mask = np.zeros(critics.shape[0], dtype=np.bool)\n",
    "mask[itest] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False,  True])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12802,\n",
       " 15048,\n",
       " 6261,\n",
       " 368,\n",
       " 9826,\n",
       " 9611,\n",
       " 1034,\n",
       " 12141,\n",
       " 8281,\n",
       " 13890,\n",
       " 5379,\n",
       " 14647,\n",
       " 14174,\n",
       " 11811,\n",
       " 8049,\n",
       " 6617,\n",
       " 13978,\n",
       " 12961,\n",
       " 48,\n",
       " 12006,\n",
       " 9579,\n",
       " 683,\n",
       " 7836,\n",
       " 14476,\n",
       " 9831,\n",
       " 6796,\n",
       " 1094,\n",
       " 11856,\n",
       " 7416,\n",
       " 1497,\n",
       " 13670,\n",
       " 2774,\n",
       " 10399,\n",
       " 1445,\n",
       " 7106,\n",
       " 1452,\n",
       " 14149,\n",
       " 3584,\n",
       " 4843,\n",
       " 4035,\n",
       " 15352,\n",
       " 9705,\n",
       " 6212,\n",
       " 8398,\n",
       " 12033,\n",
       " 8887,\n",
       " 12628,\n",
       " 6798,\n",
       " 11918,\n",
       " 9233,\n",
       " 4332,\n",
       " 848,\n",
       " 9493,\n",
       " 2204,\n",
       " 10318,\n",
       " 4040,\n",
       " 5471,\n",
       " 14111,\n",
       " 14935,\n",
       " 2524,\n",
       " 10011,\n",
       " 2170,\n",
       " 13273,\n",
       " 8089,\n",
       " 14512,\n",
       " 12552,\n",
       " 1299,\n",
       " 15015,\n",
       " 2729,\n",
       " 11864,\n",
       " 1238,\n",
       " 14374,\n",
       " 6202,\n",
       " 10743,\n",
       " 10807,\n",
       " 10539,\n",
       " 12172,\n",
       " 11426,\n",
       " 10835,\n",
       " 8239,\n",
       " 7281,\n",
       " 10584,\n",
       " 11866,\n",
       " 5016,\n",
       " 4618,\n",
       " 12765,\n",
       " 10288,\n",
       " 13583,\n",
       " 14783,\n",
       " 3209,\n",
       " 15085,\n",
       " 11422,\n",
       " 14067,\n",
       " 13648,\n",
       " 12007,\n",
       " 7572,\n",
       " 1003,\n",
       " 960,\n",
       " 6502,\n",
       " 9003,\n",
       " 8794,\n",
       " 13680,\n",
       " 12125,\n",
       " 5672,\n",
       " 3817,\n",
       " 9877,\n",
       " 13985,\n",
       " 9731,\n",
       " 12171,\n",
       " 4669,\n",
       " 9341,\n",
       " 4403,\n",
       " 10506,\n",
       " 12831,\n",
       " 11191,\n",
       " 10881,\n",
       " 9328,\n",
       " 12273,\n",
       " 4968,\n",
       " 2264,\n",
       " 12629,\n",
       " 14832,\n",
       " 14544,\n",
       " 6395,\n",
       " 9441,\n",
       " 2402,\n",
       " 5516,\n",
       " 14676,\n",
       " 5623,\n",
       " 5583,\n",
       " 10828,\n",
       " 6054,\n",
       " 2257,\n",
       " 10713,\n",
       " 11758,\n",
       " 14881,\n",
       " 5422,\n",
       " 15273,\n",
       " 8733,\n",
       " 4049,\n",
       " 2004,\n",
       " 2353,\n",
       " 4730,\n",
       " 10264,\n",
       " 3571,\n",
       " 11524,\n",
       " 7010,\n",
       " 392,\n",
       " 15457,\n",
       " 6451,\n",
       " 10668,\n",
       " 2392,\n",
       " 3750,\n",
       " 4110,\n",
       " 13788,\n",
       " 10477,\n",
       " 2342,\n",
       " 1304,\n",
       " 15560,\n",
       " 11480,\n",
       " 13034,\n",
       " 8314,\n",
       " 10737,\n",
       " 15554,\n",
       " 9437,\n",
       " 1791,\n",
       " 5393,\n",
       " 10388,\n",
       " 11252,\n",
       " 715,\n",
       " 3746,\n",
       " 898,\n",
       " 15391,\n",
       " 2742,\n",
       " 12217,\n",
       " 3473,\n",
       " 7401,\n",
       " 9563,\n",
       " 8768,\n",
       " 10632,\n",
       " 508,\n",
       " 135,\n",
       " 14443,\n",
       " 230,\n",
       " 2989,\n",
       " 6081,\n",
       " 4423,\n",
       " 5120,\n",
       " 1861,\n",
       " 2534,\n",
       " 2346,\n",
       " 3740,\n",
       " 6072,\n",
       " 2748,\n",
       " 13942,\n",
       " 12302,\n",
       " 7445,\n",
       " 11517,\n",
       " 4835,\n",
       " 9863,\n",
       " 10345,\n",
       " 4824,\n",
       " 13056,\n",
       " 8517,\n",
       " 11247,\n",
       " 6775,\n",
       " 7982,\n",
       " 4744,\n",
       " 9642,\n",
       " 7771,\n",
       " 7067,\n",
       " 2915,\n",
       " 14522,\n",
       " 8501,\n",
       " 5698,\n",
       " 3339,\n",
       " 4517,\n",
       " 11110,\n",
       " 7494,\n",
       " 13524,\n",
       " 9321,\n",
       " 7954,\n",
       " 3156,\n",
       " 1682,\n",
       " 2688,\n",
       " 12343,\n",
       " 7379,\n",
       " 1496,\n",
       " 11070,\n",
       " 13352,\n",
       " 14677,\n",
       " 1063,\n",
       " 11461,\n",
       " 10761,\n",
       " 10480,\n",
       " 999,\n",
       " 8138,\n",
       " 2679,\n",
       " 363,\n",
       " 9149,\n",
       " 15510,\n",
       " 8315,\n",
       " 2383,\n",
       " 3110,\n",
       " 2492,\n",
       " 9708,\n",
       " 10782,\n",
       " 9381,\n",
       " 7716,\n",
       " 15118,\n",
       " 14618,\n",
       " 11402,\n",
       " 3383,\n",
       " 8441,\n",
       " 15309,\n",
       " 1906,\n",
       " 1073,\n",
       " 2746,\n",
       " 3741,\n",
       " 10764,\n",
       " 8631,\n",
       " 8953,\n",
       " 10407,\n",
       " 12277,\n",
       " 7816,\n",
       " 3403,\n",
       " 4965,\n",
       " 4939,\n",
       " 2738,\n",
       " 13917,\n",
       " 5486,\n",
       " 765,\n",
       " 5202,\n",
       " 10435,\n",
       " 13205,\n",
       " 14224,\n",
       " 1239,\n",
       " 12023,\n",
       " 9243,\n",
       " 406,\n",
       " 5539,\n",
       " 3674,\n",
       " 13347,\n",
       " 2660,\n",
       " 4308,\n",
       " 15476,\n",
       " 3108,\n",
       " 9512,\n",
       " 12608,\n",
       " 13449,\n",
       " 6141,\n",
       " 8199,\n",
       " 8559,\n",
       " 2612,\n",
       " 13062,\n",
       " 1638,\n",
       " 15449,\n",
       " 10797,\n",
       " 15285,\n",
       " 15484,\n",
       " 8552,\n",
       " 5356,\n",
       " 9712,\n",
       " 11495,\n",
       " 13933,\n",
       " 11221,\n",
       " 11565,\n",
       " 12441,\n",
       " 14005,\n",
       " 13426,\n",
       " 3640,\n",
       " 4169,\n",
       " 3294,\n",
       " 8317,\n",
       " 9382,\n",
       " 5531,\n",
       " 7378,\n",
       " 9994,\n",
       " 7487,\n",
       " 1770,\n",
       " 2109,\n",
       " 1545,\n",
       " 4493,\n",
       " 5682,\n",
       " 9082,\n",
       " 12195,\n",
       " 7349,\n",
       " 8175,\n",
       " 9327,\n",
       " 3010,\n",
       " 8137,\n",
       " 2305,\n",
       " 9870,\n",
       " 2622,\n",
       " 3317,\n",
       " 8358,\n",
       " 12750,\n",
       " 8282,\n",
       " 9158,\n",
       " 15372,\n",
       " 7196,\n",
       " 8372,\n",
       " 5251,\n",
       " 3192,\n",
       " 4179,\n",
       " 11253,\n",
       " 8889,\n",
       " 13180,\n",
       " 14304,\n",
       " 7409,\n",
       " 2864,\n",
       " 8830,\n",
       " 571,\n",
       " 1571,\n",
       " 4727,\n",
       " 12063,\n",
       " 14737,\n",
       " 15080,\n",
       " 10834,\n",
       " 11200,\n",
       " 2704,\n",
       " 4482,\n",
       " 3024,\n",
       " 11841,\n",
       " 3943,\n",
       " 13945,\n",
       " 9644,\n",
       " 4930,\n",
       " 6595,\n",
       " 5639,\n",
       " 11436,\n",
       " 13030,\n",
       " 6968,\n",
       " 10090,\n",
       " 2909,\n",
       " 9044,\n",
       " 7406,\n",
       " 13517,\n",
       " 13278,\n",
       " 10572,\n",
       " 15428,\n",
       " 6270,\n",
       " 13432,\n",
       " 13230,\n",
       " 12453,\n",
       " 6211,\n",
       " 1131,\n",
       " 3947,\n",
       " 6299,\n",
       " 9607,\n",
       " 10976,\n",
       " 6931,\n",
       " 2044,\n",
       " 2689,\n",
       " 15117,\n",
       " 2812,\n",
       " 8384,\n",
       " 8523,\n",
       " 10767,\n",
       " 7388,\n",
       " 14063,\n",
       " 14744,\n",
       " 9960,\n",
       " 8904,\n",
       " 11202,\n",
       " 6805,\n",
       " 4129,\n",
       " 11582,\n",
       " 13212,\n",
       " 10917,\n",
       " 6781,\n",
       " 14460,\n",
       " 2040,\n",
       " 8172,\n",
       " 7570,\n",
       " 1919,\n",
       " 4231,\n",
       " 12242,\n",
       " 6218,\n",
       " 11272,\n",
       " 3564,\n",
       " 11593,\n",
       " 13520,\n",
       " 10625,\n",
       " 8274,\n",
       " 7527,\n",
       " 4615,\n",
       " 14442,\n",
       " 5366,\n",
       " 7458,\n",
       " 13580,\n",
       " 8045,\n",
       " 1366,\n",
       " 5804,\n",
       " 13930,\n",
       " 8457,\n",
       " 1432,\n",
       " 10678,\n",
       " 3385,\n",
       " 4319,\n",
       " 8996,\n",
       " 12139,\n",
       " 12652,\n",
       " 5290,\n",
       " 1459,\n",
       " 12728,\n",
       " 5999,\n",
       " 13687,\n",
       " 2961,\n",
       " 10530,\n",
       " 8115,\n",
       " 11739,\n",
       " 3513,\n",
       " 9748,\n",
       " 3851,\n",
       " 8634,\n",
       " 2508,\n",
       " 11333,\n",
       " 4579,\n",
       " 3881,\n",
       " 9337,\n",
       " 4931,\n",
       " 12241,\n",
       " 11414,\n",
       " 9119,\n",
       " 8081,\n",
       " 8766,\n",
       " 1525,\n",
       " 14538,\n",
       " 11982,\n",
       " 12890,\n",
       " 14696,\n",
       " 15223,\n",
       " 9284,\n",
       " 8446,\n",
       " 3257,\n",
       " 1624,\n",
       " 14091,\n",
       " 13166,\n",
       " 3577,\n",
       " 10849,\n",
       " 2076,\n",
       " 13764,\n",
       " 1707,\n",
       " 10773,\n",
       " 13170,\n",
       " 6607,\n",
       " 9626,\n",
       " 14022,\n",
       " 7711,\n",
       " 13948,\n",
       " 14983,\n",
       " 10395,\n",
       " 12458,\n",
       " 7295,\n",
       " 1435,\n",
       " 6702,\n",
       " 13679,\n",
       " 5795,\n",
       " 4935,\n",
       " 5142,\n",
       " 7419,\n",
       " 13642,\n",
       " 4671,\n",
       " 8124,\n",
       " 1849,\n",
       " 11716,\n",
       " 1589,\n",
       " 14666,\n",
       " 2473,\n",
       " 10182,\n",
       " 5152,\n",
       " 5647,\n",
       " 4904,\n",
       " 357,\n",
       " 15330,\n",
       " 11983,\n",
       " 4104,\n",
       " 15506,\n",
       " 11219,\n",
       " 14993,\n",
       " 15239,\n",
       " 14732,\n",
       " 7028,\n",
       " 14416,\n",
       " 7609,\n",
       " 13324,\n",
       " 13457,\n",
       " 13979,\n",
       " 6671,\n",
       " 2884,\n",
       " 375,\n",
       " 9252,\n",
       " 14158,\n",
       " 7937,\n",
       " 10701,\n",
       " 13960,\n",
       " 11039,\n",
       " 7930,\n",
       " 6784,\n",
       " 7239,\n",
       " 1470,\n",
       " 5907,\n",
       " 5649,\n",
       " 8354,\n",
       " 5439,\n",
       " 3259,\n",
       " 326,\n",
       " 7706,\n",
       " 4655,\n",
       " 10179,\n",
       " 1976,\n",
       " 8876,\n",
       " 13497,\n",
       " 2690,\n",
       " 1953,\n",
       " 9213,\n",
       " 13462,\n",
       " 1930,\n",
       " 14590,\n",
       " 13421,\n",
       " 6247,\n",
       " 13724,\n",
       " 923,\n",
       " 2857,\n",
       " 2148,\n",
       " 1247,\n",
       " 10046,\n",
       " 10305,\n",
       " 8458,\n",
       " 9436,\n",
       " 3089,\n",
       " 14619,\n",
       " 3245,\n",
       " 12914,\n",
       " 3486,\n",
       " 3772,\n",
       " 11152,\n",
       " 5823,\n",
       " 14079,\n",
       " 11004,\n",
       " 11910,\n",
       " 13311,\n",
       " 5504,\n",
       " 14802,\n",
       " 11770,\n",
       " 1702,\n",
       " 13215,\n",
       " 5815,\n",
       " 12091,\n",
       " 5299,\n",
       " 7502,\n",
       " 9821,\n",
       " 3357,\n",
       " 5346,\n",
       " 7078,\n",
       " 11125,\n",
       " 2974,\n",
       " 10,\n",
       " 3177,\n",
       " 13376,\n",
       " 2496,\n",
       " 1254,\n",
       " 8284,\n",
       " 9605,\n",
       " 14518,\n",
       " 2668,\n",
       " 7824,\n",
       " 11121,\n",
       " 708,\n",
       " 2517,\n",
       " 5528,\n",
       " 2197,\n",
       " 12670,\n",
       " 1551,\n",
       " 7785,\n",
       " 15008,\n",
       " 7443,\n",
       " 1493,\n",
       " 12487,\n",
       " 1910,\n",
       " 1644,\n",
       " 10492,\n",
       " 2028,\n",
       " 6943,\n",
       " 1995,\n",
       " 3297,\n",
       " 6278,\n",
       " 10756,\n",
       " 5690,\n",
       " 3326,\n",
       " 11951,\n",
       " 5803,\n",
       " 14573,\n",
       " 9914,\n",
       " 15209,\n",
       " 3589,\n",
       " 5034,\n",
       " 14833,\n",
       " 11460,\n",
       " 6646,\n",
       " 6995,\n",
       " 15178,\n",
       " 10483,\n",
       " 9859,\n",
       " 6901,\n",
       " 12003,\n",
       " 6616,\n",
       " 4246,\n",
       " 1192,\n",
       " 9211,\n",
       " 6907,\n",
       " 8170,\n",
       " 4827,\n",
       " 11700,\n",
       " 5974,\n",
       " 4322,\n",
       " 1583,\n",
       " 7631,\n",
       " 11245,\n",
       " 348,\n",
       " 1802,\n",
       " 9291,\n",
       " 12772,\n",
       " 5634,\n",
       " 11629,\n",
       " 2875,\n",
       " 13461,\n",
       " 10106,\n",
       " 836,\n",
       " 10198,\n",
       " 6739,\n",
       " 11080,\n",
       " 10189,\n",
       " 6992,\n",
       " 3870,\n",
       " 1439,\n",
       " 11713,\n",
       " 4446,\n",
       " 5036,\n",
       " 6132,\n",
       " 8629,\n",
       " 2693,\n",
       " 3074,\n",
       " 10759,\n",
       " 14579,\n",
       " 3977,\n",
       " 6494,\n",
       " 7197,\n",
       " 15063,\n",
       " 2751,\n",
       " 6258,\n",
       " 8535,\n",
       " 3988,\n",
       " 2441,\n",
       " 462,\n",
       " 5523,\n",
       " 12374,\n",
       " 14815,\n",
       " 3494,\n",
       " 239,\n",
       " 4781,\n",
       " 6130,\n",
       " 7274,\n",
       " 10188,\n",
       " 4144,\n",
       " 9408,\n",
       " 10877,\n",
       " 9317,\n",
       " 384,\n",
       " 11532,\n",
       " 6928,\n",
       " 8417,\n",
       " 3327,\n",
       " 3282,\n",
       " 8782,\n",
       " 12370,\n",
       " 6569,\n",
       " 5085,\n",
       " 14816,\n",
       " 169,\n",
       " 1369,\n",
       " 7535,\n",
       " 3955,\n",
       " 11061,\n",
       " 9850,\n",
       " 8527,\n",
       " 165,\n",
       " 14436,\n",
       " 12604,\n",
       " 1052,\n",
       " 161,\n",
       " 1032,\n",
       " 10257,\n",
       " 5080,\n",
       " 4209,\n",
       " 11499,\n",
       " 15236,\n",
       " 12373,\n",
       " 11098,\n",
       " 10505,\n",
       " 4736,\n",
       " 222,\n",
       " 7447,\n",
       " 4679,\n",
       " 6454,\n",
       " 1169,\n",
       " 1837,\n",
       " 11444,\n",
       " 14958,\n",
       " 5404,\n",
       " 2777,\n",
       " 13167,\n",
       " 10289,\n",
       " 14419,\n",
       " 10447,\n",
       " 7430,\n",
       " 4926,\n",
       " 6723,\n",
       " 3496,\n",
       " 1465,\n",
       " 3188,\n",
       " 11193,\n",
       " 3330,\n",
       " 7839,\n",
       " 10271,\n",
       " 12857,\n",
       " 8646,\n",
       " 6363,\n",
       " 10499,\n",
       " 6497,\n",
       " 1394,\n",
       " 4223,\n",
       " 1735,\n",
       " 4402,\n",
       " 5430,\n",
       " 2039,\n",
       " 13009,\n",
       " 6954,\n",
       " 5945,\n",
       " 512,\n",
       " 9482,\n",
       " 5610,\n",
       " 14142,\n",
       " 9132,\n",
       " 11017,\n",
       " 4987,\n",
       " 10970,\n",
       " 3855,\n",
       " 11803,\n",
       " 3660,\n",
       " 10413,\n",
       " 13065,\n",
       " 9332,\n",
       " 14495,\n",
       " 3963,\n",
       " 14427,\n",
       " 3269,\n",
       " 5337,\n",
       " 5949,\n",
       " 8144,\n",
       " 3783,\n",
       " 6915,\n",
       " 2110,\n",
       " 13326,\n",
       " 664,\n",
       " 11857,\n",
       " 4054,\n",
       " 12589,\n",
       " 10675,\n",
       " 6320,\n",
       " 15022,\n",
       " 8456,\n",
       " 3069,\n",
       " 2787,\n",
       " 13222,\n",
       " 11483,\n",
       " 10578,\n",
       " 9496,\n",
       " 11836,\n",
       " 1980,\n",
       " 2708,\n",
       " 6924,\n",
       " 5963,\n",
       " 9230,\n",
       " 3207,\n",
       " 12483,\n",
       " 5297,\n",
       " 11198,\n",
       " 1331,\n",
       " 2998,\n",
       " 7027,\n",
       " 8491,\n",
       " 4031,\n",
       " 11411,\n",
       " 9886,\n",
       " 8440,\n",
       " 4663,\n",
       " 12015,\n",
       " 10676,\n",
       " 6034,\n",
       " 13428,\n",
       " 8749,\n",
       " 9138,\n",
       " 8401,\n",
       " 1262,\n",
       " 8643,\n",
       " 3410,\n",
       " 2356,\n",
       " 3287,\n",
       " 9216,\n",
       " 2436,\n",
       " 8614,\n",
       " 14080,\n",
       " 6823,\n",
       " 9880,\n",
       " 6662,\n",
       " 6475,\n",
       " 8268,\n",
       " 5954,\n",
       " 5554,\n",
       " 15090,\n",
       " 1413,\n",
       " 2284,\n",
       " 12829,\n",
       " 2362,\n",
       " 10785,\n",
       " 1189,\n",
       " 11763,\n",
       " 5116,\n",
       " 1513,\n",
       " 7046,\n",
       " 14700,\n",
       " 14393,\n",
       " 2681,\n",
       " 6361,\n",
       " 14046,\n",
       " 8616,\n",
       " 1733,\n",
       " 10112,\n",
       " 6910,\n",
       " 15341,\n",
       " 15146,\n",
       " 3203,\n",
       " 6948,\n",
       " 12685,\n",
       " 10432,\n",
       " 1061,\n",
       " 11438,\n",
       " 8632,\n",
       " 5798,\n",
       " 521,\n",
       " 1074,\n",
       " 15233,\n",
       " 8648,\n",
       " 8696,\n",
       " 7389,\n",
       " 2457,\n",
       " 11238,\n",
       " 13160,\n",
       " 6717,\n",
       " 9608,\n",
       " 10886,\n",
       " 1526,\n",
       " 4633,\n",
       " 11143,\n",
       " 9231,\n",
       " 5965,\n",
       " 2867,\n",
       " 13850,\n",
       " 15107,\n",
       " 8463,\n",
       " 3488,\n",
       " 11671,\n",
       " 8964,\n",
       " 13075,\n",
       " 5855,\n",
       " 4504,\n",
       " 1377,\n",
       " 8207,\n",
       " 4895,\n",
       " 14633,\n",
       " 30,\n",
       " 9460,\n",
       " 12310,\n",
       " 8954,\n",
       " 11805,\n",
       " 8468,\n",
       " 9957,\n",
       " 2908,\n",
       " 4959,\n",
       " 793,\n",
       " 13946,\n",
       " 10704,\n",
       " 6145,\n",
       " 6476,\n",
       " 3471,\n",
       " 8325,\n",
       " 15065,\n",
       " 3591,\n",
       " 15541,\n",
       " 5762,\n",
       " 11872,\n",
       " 3389,\n",
       " 1011,\n",
       " 7383,\n",
       " 6712,\n",
       " 13237,\n",
       " 11911,\n",
       " 47,\n",
       " 15424,\n",
       " 14332,\n",
       " 5597,\n",
       " 13200,\n",
       " 14572,\n",
       " 7905,\n",
       " 4309,\n",
       " 9167,\n",
       " 2295,\n",
       " 4732,\n",
       " 9662,\n",
       " 12235,\n",
       " 12438,\n",
       " 4695,\n",
       " 4073,\n",
       " 12779,\n",
       " 3560,\n",
       " 11654,\n",
       " 7807,\n",
       " 4367,\n",
       " 8304,\n",
       " 7016,\n",
       " 13539,\n",
       " 15304,\n",
       " 15463,\n",
       " 5291,\n",
       " 4716,\n",
       " 4291,\n",
       " 13999,\n",
       " 3311,\n",
       " 4822,\n",
       " 6877,\n",
       " 5233,\n",
       " 3840,\n",
       " 6487,\n",
       " 3308,\n",
       " 12350,\n",
       " 4738,\n",
       " 5581,\n",
       " 1096,\n",
       " 3190,\n",
       " 1148,\n",
       " 5895,\n",
       " 13749,\n",
       " 2887,\n",
       " 11416,\n",
       " 3665,\n",
       " 7095,\n",
       " 7056,\n",
       " 12050,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set IV</h3>\n",
    "\n",
    "<p><b>Exercise:</b> What does using the function `log_likelihood` as the score mean? What are we trying to optimize for?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Without writing any code, what do you think would happen if you choose a value of $\\alpha$ that is too high?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Using the skeleton code below, find the best values of the parameter `alpha`, and use the value of `min_df` you chose in the previous exercise set. Use the `cv_score` function above with the `log_likelihood` function for scoring.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### log_likelihood is a scoring function to give us confidence in our modelling technique adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "new score -943.1653590595895\n",
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "new score -713.1185603792624\n",
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "new score -1240.9729164662567\n",
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "new score -1394.6832722786874\n",
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "new score -1354.1572527177195\n",
      "Best alpha: 1\n",
      "Best score: -713.1185603792624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "# best_min_df = None # YOUR TURN: put your value of min_df here.\n",
    "best_min_df = 1\n",
    "    \n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "maxscore=-np.inf\n",
    "new_score=-np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df)       \n",
    "    Xthis, ythis = make_xy(critics, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    # your turn\n",
    "    clfnb = MultinomialNB(alpha=alpha)\n",
    "    new_score = cv_score(clfnb, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    print(\"new score {}\".format(new_score))\n",
    "    if new_score > maxscore:\n",
    "        maxscore = new_score\n",
    "        best_alpha = alpha\n",
    "print(\"Best alpha: {}\".format(best_alpha))\n",
    "print(\"Best score: {}\".format(maxscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### With higher alpha the scores start deteriorating as seen above for alpha=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set V: Working with the Best Parameters</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Using the best value of  `alpha` you just found, calculate the accuracy on the training and test sets. Is this classifier better? Why (not)?</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "Accuracy on training data: 0.936389\n",
      "Accuracy on test data:     0.736320\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "X, y = make_xy(critics, vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "# clf = MultinomialNB(alpha=10).fit(xtrain, ytrain)\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "\n",
    "#your turn. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1979 2252]\n",
      " [ 620 6041]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, clf.predict(xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the strongly predictive features?\n",
    "\n",
    "We use a neat trick to identify strongly predictive features (i.e. words). \n",
    "\n",
    "* first, create a data set such that each row has exactly one feature. This is represented by the identity matrix.\n",
    "* use the trained classifier to make predictions on this matrix\n",
    "* sort the rows by predicted probabilities, and pick the top and bottom $K$ rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??np.argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-124df652efe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ind' is not defined"
     ]
    }
   ],
   "source": [
    "ind[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22417"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(fresh | word)\n",
      "        surprisingly 0.96\n",
      "             delight 0.95\n",
      "         intelligent 0.94\n",
      "          remarkable 0.94\n",
      "         masterpiece 0.94\n",
      "             likable 0.94\n",
      "        exhilarating 0.93\n",
      "             lyrical 0.93\n",
      "             rousing 0.93\n",
      "               truth 0.93\n",
      "Bad words\t     P(fresh | word)\n",
      "             leaving 0.12\n",
      "                  90 0.12\n",
      "           pointless 0.11\n",
      "              except 0.11\n",
      "           misguided 0.10\n",
      "             muddled 0.10\n",
      "     disappointingly 0.09\n",
      "                lame 0.09\n",
      "          uninspired 0.09\n",
      "            tiresome 0.09\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "probs1 = clf.predict_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Good words\\t     P(fresh | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(fresh | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5644468 , -1.18644738, -1.18644738, ..., -1.18644738,\n",
       "       -0.75973614, -0.75973614])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03680885, 0.04916154, 0.05715323, 0.05907383, 0.06112801,\n",
       "       0.06333019, 0.06824753, 0.07399278, 0.07399278, 0.07399278])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1[ind[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87549268, 0.87549268, 0.88777428, 0.88777428, 0.89785047,\n",
       "       0.89785047, 0.90626635, 0.90626635, 0.90626635, 0.91340105])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1[ind[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VI</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Why does this method work? What does the probability for each row in the identity matrix represent</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above exercise is an example of *feature selection*. There are many other feature selection methods. A list of feature selection methods available in `sklearn` is [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection). The most common feature selection technique for text mining is the chi-squared $\\left( \\chi^2 \\right)$ [method](http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Errors\n",
    "\n",
    "We can see mis-predictions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3248)\t1\n",
      "  (0, 10566)\t1\n",
      "  (0, 2784)\t1\n",
      "  (0, 6494)\t1\n",
      "  (0, 1767)\t1\n",
      "  (0, 18904)\t1\n",
      "  (0, 17231)\t1\n",
      "  (0, 17943)\t1\n",
      "  (0, 18757)\t1\n",
      "  (0, 14940)\t1\n",
      "  (0, 13657)\t1\n",
      "  (0, 10555)\t1\n",
      "  (0, 21732)\t1\n",
      "  (0, 4386)\t1\n",
      "  (0, 22330)\t1\n",
      "  (0, 19914)\t1\n",
      "  (0, 6856)\t1\n",
      "  (0, 891)\t2\n",
      "  (0, 5251)\t1\n",
      "  (0, 4003)\t1\n",
      "  (0, 9950)\t1\n",
      "  (0, 10176)\t1\n",
      "  (0, 18252)\t1\n",
      "  (1, 3835)\t1\n",
      "  (1, 10463)\t1\n",
      "  :\t:\n",
      "  (15559, 19917)\t3\n",
      "  (15559, 10566)\t1\n",
      "  (15559, 18904)\t1\n",
      "  (15559, 19914)\t1\n",
      "  (15559, 891)\t1\n",
      "  (15560, 19057)\t1\n",
      "  (15560, 10261)\t1\n",
      "  (15560, 8719)\t1\n",
      "  (15560, 362)\t1\n",
      "  (15560, 5294)\t1\n",
      "  (15560, 6527)\t1\n",
      "  (15560, 5747)\t1\n",
      "  (15560, 15259)\t1\n",
      "  (15560, 20129)\t1\n",
      "  (15560, 9087)\t1\n",
      "  (15560, 10363)\t1\n",
      "  (15560, 11404)\t1\n",
      "  (15560, 21212)\t1\n",
      "  (15560, 10891)\t1\n",
      "  (15560, 20187)\t1\n",
      "  (15560, 13584)\t1\n",
      "  (15560, 7729)\t1\n",
      "  (15560, 19917)\t2\n",
      "  (15560, 10555)\t1\n",
      "  (15560, 891)\t1\n",
      "  (1028, 0)\t1\n",
      "  (3215, 0)\t1\n",
      "  (4740, 0)\t1\n",
      "  (4741, 0)\t1\n",
      "  (8722, 0)\t2\n",
      "  (8820, 0)\t1\n",
      "  (12118, 0)\t1\n",
      "  (4031, 1)\t1\n",
      "  (74, 2)\t1\n",
      "  (76, 2)\t1\n",
      "  (3653, 2)\t1\n",
      "  (4031, 2)\t1\n",
      "  (8023, 2)\t1\n",
      "  (10704, 2)\t1\n",
      "  (13425, 2)\t1\n",
      "  (13435, 2)\t1\n",
      "  (13668, 2)\t1\n",
      "  (13693, 2)\t1\n",
      "  (8647, 3)\t1\n",
      "  (369, 4)\t1\n",
      "  (24, 5)\t1\n",
      "  (65, 5)\t1\n",
      "  (472, 5)\t1\n",
      "  (625, 5)\t1\n",
      "  (731, 5)\t1\n",
      "  :\t:\n",
      "  (14548, 22402)\t1\n",
      "  (14551, 22402)\t1\n",
      "  (958, 22403)\t1\n",
      "  (10209, 22403)\t1\n",
      "  (6804, 22404)\t1\n",
      "  (8594, 22405)\t1\n",
      "  (14697, 22406)\t1\n",
      "  (6465, 22407)\t1\n",
      "  (4134, 22408)\t1\n",
      "  (8145, 22408)\t1\n",
      "  (4471, 22409)\t1\n",
      "  (4477, 22409)\t1\n",
      "  (4478, 22409)\t1\n",
      "  (4476, 22410)\t1\n",
      "  (8847, 22411)\t1\n",
      "  (1076, 22412)\t1\n",
      "  (3126, 22412)\t1\n",
      "  (3128, 22412)\t1\n",
      "  (9252, 22412)\t1\n",
      "  (2774, 22413)\t1\n",
      "  (3360, 22414)\t1\n",
      "  (5930, 22414)\t1\n",
      "  (10457, 22414)\t1\n",
      "  (4473, 22415)\t1\n",
      "  (8518, 22416)\t1\n",
      "Mis-predicted Rotten quotes\n",
      "---------------------------\n",
      "A new private-eye melodrama that celebrates not only a time and a place (Los Angeles) but also a kind of criminality that to us jaded souls today appears to be nothing worse than an eccentric form of legitimate private enterprise.\n",
      "rotten\n",
      "\n",
      "The English Patient captivates as only the grandest and most consuming passions can.\n",
      "fresh\n",
      "\n",
      "Like its politicians, the movie is savvy about using whatever it takes to achieve a goal.\n",
      "rotten\n",
      "\n",
      "Affectionate, often clever and unflaggingly funny!\n",
      "rotten\n",
      "\n",
      "[Aliens] is a spectacular example of state-of-the-art science fiction simply because it never tries to be overtly spectacular.\n",
      "rotten\n",
      "\n",
      "Mis-predicted Fresh quotes\n",
      "--------------------------\n",
      "Features heavy-handed exposition, repetitive, maudlin flashbacks, uneven performances and endless sermonising.\n",
      "fresh\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-417729eca8cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbad_fresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfresh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda33\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda33\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m                 \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda33\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1526\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "x, y = make_xy(critics, vectorizer)\n",
    "\n",
    "prob = clf.predict_proba(x)[:, 0]\n",
    "predict = clf.predict(x)\n",
    "\n",
    "bad_rotten = np.argsort(prob[y == 0])[:5]\n",
    "bad_fresh = np.argsort(prob[y == 1])[-5:]\n",
    "\n",
    "print(\"Mis-predicted Rotten quotes\")\n",
    "print('---------------------------')\n",
    "for row in bad_rotten: \n",
    "    print(critics[y == 1].quote.iloc[row])\n",
    "    print(critics.fresh.iloc[row]) \n",
    "    print(\"\")\n",
    "\n",
    "print(\"Mis-predicted Fresh quotes\")\n",
    "print('--------------------------')\n",
    "for row in bad_fresh:\n",
    "    print(critics[y == 0].quote.iloc[row])\n",
    "    print(critics.fresh.iloc[row])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(prob[y==0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        critic  fresh    imdb publication                                              quote review_date  rtid      title\n",
       "1  Derek Adams  fresh  114709    Time Out  So ingenious in concept, design and execution ...  2009-10-04  9559  Toy story"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics.iloc[np.argsort(prob[y==0][0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    So ingenious in concept, design and execution ...\n",
      "Name: quote, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(critics.quote.iloc[np.argsort(prob[y==0][0])])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VII: Predicting the Freshness for a New Review</h3>\n",
    "<br/>\n",
    "<div>\n",
    "<b>Exercise:</b>\n",
    "<ul>\n",
    "<li> Using your best trained classifier, predict the freshness of the following sentence: *'This movie is not remarkable, touching, or superb in any way'*\n",
    "<li> Is the result what you'd expect? Why (not)?\n",
    "</ul>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               quote\n",
      "0  This movie is not remarkable, touching, or sup...\n",
      "[0.12256488 0.02481466 0.05083574 ... 0.00969561 0.0889243  0.02426903]\n",
      "[1 1 1 ... 1 1 1]\n",
      "(15561,)\n",
      "   fresh  count\n",
      "0      0   4208\n",
      "1      1  11353\n",
      "['any' 'in' 'is' 'movie' 'not' 'or' 'remarkable' 'superb' 'this'\n",
      " 'touching' 'way']\n"
     ]
    }
   ],
   "source": [
    "#your turn\n",
    "new_df = pd.DataFrame(columns=['quote'])\n",
    "new_df['quote'] = ['This movie is not remarkable, touching, or superb in any way']\n",
    "print(new_df)\n",
    "\n",
    "# X = make_xy(new_df, vectorizer)\n",
    "\n",
    "if vectorizer is None:\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(new_df.quote)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    print(X)\n",
    "\n",
    "\n",
    "prob = clf.predict_proba(X)[:, 0]\n",
    "predict = clf.predict(X)\n",
    "\n",
    "print(prob)\n",
    "print(predict)\n",
    "print(predict.shape)\n",
    "\n",
    "predict_df=pd.DataFrame(predict, columns=['fresh'])\n",
    "predict_df = predict_df.groupby([\"fresh\"]).size().reset_index(name='count')\n",
    "print(predict_df)\n",
    "\n",
    "\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "print(words)\n",
    "\n",
    "# ind = np.argsort(prob)\n",
    "# print(ind)\n",
    "\n",
    "# good_words = words[:5]\n",
    "# bad_words = words[-5:]\n",
    "\n",
    "# good_prob = probs[:5]\n",
    "# bad_prob = probs[-5:]\n",
    "\n",
    "# print(\"Good words\\t     P(fresh | word)\")\n",
    "# for w, p in zip(good_words, good_prob):\n",
    "#     print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "# print(\"Bad words\\t     P(fresh | word)\")\n",
    "# for w, p in zip(bad_words, bad_prob):\n",
    "#     print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As seen above the fresh 1 count is higher (11353) indicating that this quote is more fresh than rotten, which is incorrect.  The negation at the beginning of the quote is hard to catch by such standardized models unless a certain weightage is given to each word i.e. 'not' getting the most weight in this sentence and thereby pulling it towards a 'rotten' rating would be the way to redesign the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: TF-IDF Weighting for Term Importance\n",
    "\n",
    "TF-IDF stands for \n",
    "\n",
    "`Term-Frequency X Inverse Document Frequency`.\n",
    "\n",
    "In the standard `CountVectorizer` model above, we used just the term frequency in a document of words in our vocabulary. In TF-IDF, we weight this term frequency by the inverse of its popularity in all documents. For example, if the word \"movie\" showed up in all the documents, it would not have much predictive value. It could actually be considered a stopword. By weighing its counts by 1 divided by its overall frequency, we downweight it. We can then use this TF-IDF weighted features as inputs to any classifier. **TF-IDF is essentially a measure of term importance, and of how discriminative a word is in a corpus.** There are a variety of nuances involved in computing TF-IDF, mainly involving where to add the smoothing term to avoid division by 0, or log of 0 errors. The formula for TF-IDF in `scikit-learn` differs from that of most textbooks: \n",
    "\n",
    "$$\\mbox{TF-IDF}(t, d) = \\mbox{TF}(t, d)\\times \\mbox{IDF}(t) = n_{td} \\log{\\left( \\frac{\\vert D \\vert}{\\vert d : t \\in d \\vert} + 1 \\right)}$$\n",
    "\n",
    "where $n_{td}$ is the number of times term $t$ occurs in document $d$, $\\vert D \\vert$ is the number of documents, and $\\vert d : t \\in d \\vert$ is the number of documents that contain $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/dev/modules/feature_extraction.html#text-feature-extraction\n",
    "# http://scikit-learn.org/dev/modules/classes.html#text-feature-extraction-ref\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "Xtfidf=tfidfvectorizer.fit_transform(critics.quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set VIII: Enrichment</h3>\n",
    "\n",
    "<p>\n",
    "There are several additional things we could try. Try some of these as exercises:\n",
    "<ol>\n",
    "<li> Build a Naive Bayes model where the features are n-grams instead of words. N-grams are phrases containing n words next to each other: a bigram contains 2 words, a trigram contains 3 words, and 6-gram contains 6 words. This is useful because \"not good\" and \"so good\" mean very different things. On the other hand, as n increases, the model does not scale well since the feature set becomes more sparse.\n",
    "<li> Try a model besides Naive Bayes, one that would allow for interactions between words -- for example, a Random Forest classifier.\n",
    "<li> Try adding supplemental features -- information about genre, director, cast, etc.\n",
    "<li> Use word2vec or [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to group words into topics and use those topics for prediction.\n",
    "<li> Use TF-IDF weighting instead of word counts.\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<b>Exercise:</b> Try a few of these ideas to improve the model (or any other ideas of your own). Implement here and report on the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your turn\n",
    "pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda33]",
   "language": "python",
   "name": "conda-env-Anaconda33-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
